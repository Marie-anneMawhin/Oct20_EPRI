{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM for survival, gridsearch for alpha, fit +predict on simulated, predict on original, plot predicted scores, test regression for survival time (not good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, glob, inspect, sys\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.datasets import load_gbsg2\n",
    "import sksurv\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "import epri_mc_lib_3 as mc\n",
    "from importlib import reload\n",
    "reload(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import ShuffleSplit, GridSearchCV\n",
    "\n",
    "from sksurv.datasets import load_veterans_lung_cancer\n",
    "from sksurv.column import encode_categorical\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sksurv.svm import FastSurvivalSVM\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), '../Data/Merged_data/Survival_df.csv'),\n",
    "                  index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), '../Data/Merged_data/CopulaGAN_simulated_data_survival_2.csv'),\n",
    "                  index_col=0)\n",
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperating input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_x = real_data.iloc[:, 2:]\n",
    " # The real data input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_real_x = mc.scale_general(real_x, MinMaxScaler())[0]\n",
    "sc_real_x.index = real_x.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_real_x_sub= sc_real_x[mc.feature_selection2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulated  (Copula GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_x = data.iloc[:, 2:]\n",
    " # The simulated input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_data_x = mc.scale_general(data_x, MinMaxScaler())[0]\n",
    "sc_data_x.index = data_x.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_data_x_sub= sc_data_x[mc.feature_selection2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_y = real_data.iloc[:, 0:2]\n",
    " # The real data output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of the simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = data.iloc[:, 0:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Test, Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for the simulated data\n",
    "X_train, X_test, y_train, y_test = train_test_split(sc_data_x, data_y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Formatting y dataset for the survival ananlysis\n",
    "data_y_num=data_y.to_records(index=False)\n",
    "real_y_num=real_y.to_records(index=False)\n",
    "y_train_num = y_train.to_records(index=False)\n",
    "y_test_num = y_test.to_records(index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Survival Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide demonstrates how to use the efficient implementation of Survival Support Vector Machines, which is an extension of the standard Support Vector Machine to right-censored time-to-event data. Its main advantage is that it can account for complex, non-linear relationships between features and survival via the so-called kernel trick. A kernel function implicitly maps the input features into high-dimensional feature spaces where survival can be described by a hyperplane. This makes Survival Support Vector Machines extremely versatile and applicable to a wide a range of data. A popular example for such a kernel function is the Radial Basis Function.\n",
    "\n",
    "Survival analysis in the context of Support Vector Machines can be described in two different ways:\n",
    "\n",
    "As a ranking problem: the model learns to assign samples with shorter survival times a lower rank by considering all possible pairs of samples in the training data.\n",
    "\n",
    "As a regression problem: the model learns to directly predict the (log) survival time.\n",
    "\n",
    "In both cases, the disadvantage is that predictions cannot be easily related to standard quantities in survival analysis, namely survival function and cumulative hazard function. Moreover, they have to retain a copy of the training data to do predictions.\n",
    "\n",
    "Let’s start by taking a closer look at the Linear Survival Support Vector Machine, which does not allow selecting a specific kernel function, but can be fitted faster than the more generic Kernel Survival Support Vector Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y_num.shape[0] # Total number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y_num[\"Observed\"].sum() # Number of uncensored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the % of the censored data\n",
    "n_censored = data_y_num.shape[0] - data_y_num[\"Observed\"].sum()\n",
    "print(\"%.1f%% of records are censored\" % (n_censored / data_y_num.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "val, bins, patches = plt.hist((data_y_num[\"F_Time\"][~data_y_num[\"Observed\"]],\n",
    "                               data_y_num[\"F_Time\"][data_y_num[\"Observed\"]]),\n",
    "                              bins=30, stacked=True)\n",
    "_ = plt.legend(patches, [\"Time of Censoring\", \"Time of Failure\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inititation\n",
    "estimator = FastSurvivalSVM(max_iter=1000, tol=1e-5, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_survival_model(model, X, y):\n",
    "    '''\n",
    "    returns Harrell’s concordance index for the given estimator, X and y\n",
    "    \n",
    "    '''\n",
    "    prediction = model.predict(X)\n",
    "    result = concordance_index_censored(y['Observed'], y['F_Time'], prediction)\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyper-parameter 𝛼>0 determines the amount of regularization to apply: a smaller value increases the amount of regularization and a higher value reduces the amount of regularization. The hyper-parameter 𝑟∈[0;1] determines the trade-off between the ranking objective and the regression objective. If 𝑟=1 it reduces to the ranking objective, and if 𝑟=0 to the regression objective. If the regression objective is used, it is advised to log-transform the observed time first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': 2. ** np.arange(-12, 13, 2)}\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.5, random_state=2020)\n",
    "gcv = GridSearchCV(estimator, param_grid, scoring=score_survival_model,\n",
    "                   n_jobs=4, iid=False, refit=False,\n",
    "                   cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "gcv = gcv.fit(sc_data_x, data_y_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(gcv.best_score_, 3), gcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(gcv):\n",
    "    '''\n",
    "    Plots the performance of the Grid Search for each alpha\n",
    "    \n",
    "    '''\n",
    "    n_splits = gcv.cv.n_splits\n",
    "    cv_scores = {\"alpha\": [], \"test_score\": [], \"split\": []}\n",
    "    order = []\n",
    "    for i, params in enumerate(gcv.cv_results_[\"params\"]):\n",
    "        name = \"%.5f\" % params[\"alpha\"]\n",
    "        order.append(name)\n",
    "        for j in range(n_splits):\n",
    "            vs = gcv.cv_results_[\"split%d_test_score\" % j][i]\n",
    "            cv_scores[\"alpha\"].append(name)\n",
    "            cv_scores[\"test_score\"].append(vs)\n",
    "            cv_scores[\"split\"].append(j)\n",
    "    df = pandas.DataFrame.from_dict(cv_scores)\n",
    "    _, ax = plt.subplots(figsize=(11, 6))\n",
    "    sns.boxplot(x=\"alpha\", y=\"test_score\", data=df, order=order, ax=ax)\n",
    "    _, xtext = plt.xticks()\n",
    "    for t in xtext:\n",
    "        t.set_rotation(\"vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(gcv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal alpha value is obtained after grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best alpha value obtained from the Grid Search\n",
    "estimator.set_params(**gcv.best_params_)\n",
    "estimator.fit(sc_data_x, data_y_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to remember that only if the ranking objective is used exclusively (𝑟=1), that predictions denote risk scores, i.e. a higher predicted value indicates shorter survival, a lower value longer survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = estimator.predict(sc_data_x.iloc[:2])\n",
    "print(np.round(pred, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicted that the first sample has a higher risk than the second sample, which is in concordance with the actual survival times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing on the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_risk_scores(sc_train_x,sc_train_y,sc_test_x,sc_test_y):\n",
    "    '''\n",
    "    Predicts the Risk scores \n",
    "    \n",
    "    '''\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # The grid search begins here\n",
    "    global gcv\n",
    "    gcv = gcv.fit(sc_train_x, sc_train_y)\n",
    "    \n",
    "    # The best parameters of the grid search are: \n",
    "    print(\"The best parameters from the grid search (C-Index and alpha) :\")\n",
    "    print(round(gcv.best_score_, 3), gcv.best_params_)\n",
    "    \n",
    "    # The performance of the grid search is plotted\n",
    "    plot_performance(gcv)\n",
    "    \n",
    "    # The best alpha value obtained from the Grid Search is selected for the estimator\n",
    "    estimator.set_params(**gcv.best_params_)\n",
    "    \n",
    "    # The estimator is fitted the data\n",
    "    estimator.fit(sc_train_x, sc_train_y)\n",
    "    \n",
    "    # The prediction is done on the sample\n",
    "    pred = estimator.predict(sc_test_x)\n",
    "    \n",
    "    # Preparing the results dataframe\n",
    "    df_results=pd.DataFrame(sc_test_x.index)\n",
    "    df_results[\"Pred_Risk_Values\"]=np.round(pred, 3)\n",
    "    df_results[\"F_Time\"]=sc_test_y[\"F_Time\"]\n",
    "    df_results[\"Observed\"]=sc_test_y[\"Observed\"]\n",
    "    df_results=df_results.sort_values(by='Pred_Risk_Values')\n",
    "    \n",
    "    \n",
    "    # Plotting the predicted Risk values and comparing with the F_Time from the data to see correlation\n",
    "    plt.figure (figsize=(14,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.barh(sc_test_x.index,np.round(pred, 3))\n",
    "    plt.title('The comparison of Predicted Risk Values with F_Time from data',fontsize=20)\n",
    "    plt.ylabel(\"Sample ID\",fontsize=20)\n",
    "    plt.xlabel('Predicted Risk Values',fontsize=20)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(sc_test_x.index,sc_test_y[\"F_Time\"])\n",
    "    plt.xlabel('F_Time from data',fontsize=20)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Printing the required parameters from the calculation\n",
    "    print(\"IPCW-Index : \", mc.score_survival_model_ipcw(estimator, sc_test_x, sc_train_y, sc_test_y))\n",
    "    print(\"Predicted risk scores on the sample are :\")\n",
    "    print(np.round(pred, 3))\n",
    "    #print(\"The output data of the sample\")\n",
    "    #print(sc_test_y)\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_risk_scores(sc_data_x,data_y_num,sc_real_x,real_y_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above figure, since 'Predicted Risk values' and 'F_time' are inversely proportional, a high positive value in 'F_Time' should have more negative value in the 'Predicted Risk Value'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot using optimised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_model = FastSurvivalSVM(alpha=0.0625, fit_intercept=False, max_iter=1000,\n",
    "                optimizer='avltree', random_state=2020, rank_ratio=1.0,\n",
    "                timeit=False, tol=1e-05, verbose=False)\n",
    "tune_model.fit(sc_data_x,data_y_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tune_model.predict(sc_real_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results=pd.DataFrame(index=sc_real_x.index)\n",
    "df_results[\"Pred_Risk_Values\"]=np.round(y_pred, 3)\n",
    "df_results[\"F_Time\"]=real_y_num[\"F_Time\"]\n",
    "df_results[\"Observed\"]=real_y_num[\"Observed\"]\n",
    "df_results=df_results.sort_values(by='Pred_Risk_Values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white')\n",
    "sns.scatterplot(x='F_Time', y='Pred_Risk_Values', data=df_results, hue='Observed',\n",
    "                alpha=0.8, palette=sns.xkcd_palette(['marine blue', 'deep red'])\n",
    "               )\n",
    "plt.plot([0, 3500000], [0, -3.5], 'darkgray', lw=0.8)\n",
    "plt.xlabel('Observed survival time from NDE measurement')\n",
    "plt.ylabel('Predicted risk score')\n",
    "plt.title('SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing on the subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_scores = pred_risk_scores(sc_data_x_sub,data_y_num,sc_real_x_sub,real_y_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the regression objective is used (𝑟<1), the semantics are different, because now predictions are on the time scale and lower predicted values indicate shorter survival, higher values longer survival. Moreover, we saw from the histogram of observed times above that the distribution is skewed, therefore it is advised to log-transform the observed time before fitting a model. Here, we are going to use the transformation 𝑦′=log(1+𝑦)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s fit a model using the regression objective (𝑟=0) and compare its performance to the ranking model from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_F_Time(sc_train_x,sc_train_y,sc_test_x,sc_test_y):\n",
    "    '''\n",
    "    Predicts the F_Time \n",
    "    \n",
    "    '''\n",
    "    #log-transform the observed time before fitting a model.\n",
    "    y_log_t = sc_train_y.copy()\n",
    "    y_log_t[\"F_Time\"]= np.log1p(sc_train_y[\"F_Time\"])\n",
    "    \n",
    "    #Defining the model and fitting the data\n",
    "    ref_estimator = FastSurvivalSVM(rank_ratio=0.0, max_iter=1000, tol=1e-5, random_state=2020)\n",
    "    ref_estimator.fit(sc_train_x, y_log_t)\n",
    "    \n",
    "    #Calculating the concordance index\n",
    "    cindex = concordance_index_censored(\n",
    "    sc_train_y['Observed'],\n",
    "    sc_train_y['F_Time'],\n",
    "    -ref_estimator.predict(sc_train_x),  # flip sign to obtain risk scores\n",
    "    )\n",
    "    \n",
    "    print(\"C-Index : \", round(cindex[0], 3))\n",
    "    print(\"IPCW-Index : \", mc.score_survival_model_ipcw(ref_estimator, sc_test_x, sc_train_y, sc_test_y))\n",
    "    \n",
    "    # Predicting on the real data\n",
    "    pred_log = ref_estimator.predict(sc_test_x)\n",
    "    pred_y = np.expm1(pred_log)\n",
    "    \n",
    "    plt.scatter(sc_test_y[\"F_Time\"],pred_y)\n",
    "    plt.plot([0, 3500000], [0, 3500000])\n",
    "    plt.xlim([0, 3500000])\n",
    "    plt.ylim([0, 3500000])\n",
    "    plt.title(\"SVM Survival Regression\")\n",
    "    plt.xlabel(\"F_Time\")\n",
    "    plt.ylabel(\"Predicted F_Time\")\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that concordance_index_censored expects risk scores, therefore, we had to flip the sign of predictions. The resulting performance of the regression model is comparable to the of the ranking model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_F_Time(sc_data_x,data_y_num,sc_real_x,real_y_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_F_Time(sc_data_x_sub,data_y_num,sc_real_x_sub,real_y_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
