{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Gradient Boosting for survival prediction as AFT\n",
    "\n",
    "Test on regression trees:\n",
    "\n",
    "Gridsearch, plot performance, test changing some hyperparameters, use early stoppping to prevent overfitting, fit_predict on simulated data and predict on original, plot feature importance (gini) and permutation features importances (eli5)\n",
    "\n",
    "Test on least squares:\n",
    "Gridsearch, plot performance, test changing some hyperparameters, fit_predict on simulated data and predict on original, plot coefficients and permutation features importances (eli5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, glob, inspect, sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sksurv.metrics import concordance_index_censored, concordance_index_ipcw\n",
    "from sksurv.ensemble import ComponentwiseGradientBoostingSurvivalAnalysis\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "import epri_mc_lib_3 as mc\n",
    "from importlib import reload\n",
    "reload(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingMonitor:\n",
    "\n",
    "    def __init__(self, window_size, max_iter_without_improvement):\n",
    "        self.window_size = window_size\n",
    "        self.max_iter_without_improvement = max_iter_without_improvement\n",
    "        self._best_step = -1\n",
    "\n",
    "    def __call__(self, iteration, estimator, args):\n",
    "        # continue training for first self.window_size iterations\n",
    "        if iteration < self.window_size:\n",
    "            return False\n",
    "\n",
    "        # compute average improvement in last self.window_size iterations.\n",
    "        # oob_improvement_ is the different in negative log partial likelihood\n",
    "        # between the previous and current iteration.\n",
    "        start = iteration - self.window_size + 1\n",
    "        end = iteration + 1\n",
    "        improvement = np.mean(estimator.oob_improvement_[start:end])\n",
    "\n",
    "        if improvement > 1e-6:\n",
    "            self._best_step = iteration\n",
    "            return False  # continue fitting\n",
    "\n",
    "        # stop fitting if there was no improvement\n",
    "        # in last max_iter_without_improvement iterations\n",
    "        diff = iteration - self._best_step\n",
    "        return diff >= self.max_iter_without_improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), '../Data/Merged_data/CopulaGAN_simulated_data_survival_2.csv'),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = data.iloc[:, 2:]\n",
    "data_y = data.iloc[:, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_real = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), '../Data/Merged_data/Survival_df.csv'),\n",
    "                  index_col=0)\n",
    "\n",
    "real_x = data_real.iloc[:, 2:]\n",
    "real_y_pre = data_real.iloc[:, 0:2]\n",
    "real_y = real_y_pre.to_records(index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_pre, y_test_pre = train_test_split(\n",
    "    data_x, data_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_pre.to_records(index=False)\n",
    "y_test = y_test_pre.to_records(index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerated Time Failure models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concordance index is defined as the proportion of all comparable pairs in which the predictions and outcomes are concordant.\n",
    "We also choose to use concordance_index_ipcw as a metric as while the difference between concordance_index_ipcw and concordance_index_censored is negligible for small amounts of censoring, when analyzing survival data with moderate to high amounts of censoring the CI_censored is over confident.\n",
    "\n",
    "We chose to run the GB with 'ipcwls' ( The loss ‘ipcwls’ refers to inverse-probability of censoring weighted least squares error.) as with this method we can return *time to event* and not only log hazard ratio and they adjust for censoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting with regression trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we tested on a single split or 'stump' and print the resulting concordance index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stump = GradientBoostingSurvivalAnalysis(loss='ipcwls',\n",
    "    n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42\n",
    ")\n",
    "stump.fit(X_train, y_train)\n",
    "cindex = stump.score(X_test, y_test)\n",
    "\n",
    "print(round(cindex, 3))\n",
    "mc.score_survival_model_ipcw(stump, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'learning_rate': [0.01, 0.05, 0.1],\n",
    "              'n_estimators': [250, 500, 750, 1000, 1250],\n",
    "              'max_depth': [2, 3, 4],\n",
    "              'min_impurity_decrease': [0, 0.01],\n",
    "              'subsample': [0.4, 0.5, 0.6]\n",
    "             }\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.5, random_state=0) #to use first to refine search\n",
    "GSCV_tree = GridSearchCV(GradientBoostingSurvivalAnalysis(loss='ipcwls', random_state=42), param_grid, \n",
    "                         scoring=mc.score_survival_model,\n",
    "                   n_jobs=4, refit=False,\n",
    "                   cv=cv, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSCV_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSCV_tree.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_tree = GradientBoostingSurvivalAnalysis(\n",
    "    loss='ipcwls',\n",
    "    subsample=0.4,\n",
    "    min_impurity_decrease=0.01,\n",
    "    learning_rate=0.01, \n",
    "    max_depth=2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing several n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores =  pd.DataFrame()\n",
    "for n_estimators in range(1, 3000, 100):\n",
    "    GB_tree.set_params(n_estimators=n_estimators)\n",
    "    GB_tree.fit(X_train, y_train)\n",
    "    results = pd.DataFrame({'n_estimators': n_estimators,\n",
    "                           'ci_train': GB_tree.score(X_train, y_train),\n",
    "                            'ci_test': GB_tree.score(X_test, y_test)\n",
    "                           }, index=[0])\n",
    "    scores = scores.append(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting vs n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid')\n",
    "scores.set_index('n_estimators').plot()\n",
    "plt.xlabel('n_estimator')\n",
    "plt.ylabel('concordance index')\n",
    "plt.title('Gradient boosting with regression trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [i * 5 for i in range(1, 21)]\n",
    "\n",
    "estimators = {\n",
    "    \"no regularization\": GradientBoostingSurvivalAnalysis(\n",
    "    min_impurity_decrease=0.01,\n",
    "        learning_rate=1.0, max_depth=2, random_state=0\n",
    "    ),\n",
    "    \"learning rate\": GradientBoostingSurvivalAnalysis(\n",
    "    min_impurity_decrease=0.01,\n",
    "        learning_rate=0.1, max_depth=2, random_state=0\n",
    "    ),\n",
    "    \"dropout\": GradientBoostingSurvivalAnalysis(\n",
    "    min_impurity_decrease=0.01,\n",
    "        learning_rate=1.0, dropout_rate=0.1, max_depth=2, random_state=0\n",
    "    ),\n",
    "    \"subsample\": GradientBoostingSurvivalAnalysis(\n",
    "    min_impurity_decrease=0.01,\n",
    "        learning_rate=1.0, subsample=0.5, max_depth=2, random_state=0\n",
    "    ),\n",
    "}\n",
    "\n",
    "scores_reg = {k: [] for k in estimators.keys()}\n",
    "scores_train_reg = {k: [] for k in estimators.keys()}\n",
    "\n",
    "for n in n_estimators:\n",
    "    for name, est in estimators.items():\n",
    "        est.set_params(n_estimators=n)\n",
    "        est.fit(X_train, y_train)\n",
    "        cindex = est.score(X_test, y_test)\n",
    "        cindex_train = est.score(X_train, y_train)\n",
    "        scores_reg[name].append(cindex)\n",
    "        scores_train_reg[name].append(cindex_train)\n",
    "        \n",
    "scores_res = pd.DataFrame(scores_reg, index=n_estimators)\n",
    "scores_train_reg = pd.DataFrame(scores_train_reg, index=n_estimators)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = scores_res.plot(xlabel=\"n_estimators\", ylabel=\"concordance index\")\n",
    "ax.grid(True)\n",
    "plt.title('Test')\n",
    "plt.ylim(0.84, 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = scores_train_reg.plot(xlabel=\"n_estimators\", ylabel=\"concordance index\")\n",
    "ax.grid(True)\n",
    "plt.title('Train')\n",
    "plt.ylim(0.84, 0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stoppping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_tree_ES = GradientBoostingSurvivalAnalysis(\n",
    "    loss='ipcwls',\n",
    "    n_estimators=1250,\n",
    "    subsample=0.4,\n",
    "    min_impurity_decrease=0.01,\n",
    "    learning_rate=0.01, \n",
    "    max_depth=2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "monitor = EarlyStoppingMonitor(25, 100)\n",
    "\n",
    "GB_tree_ES.fit(X_train, y_train, monitor=monitor)\n",
    "\n",
    "print(\"Fitted base learners:\", GB_tree_ES.n_estimators_)\n",
    "\n",
    "cindex = GB_tree_ES.score(X_test, y_test)\n",
    "cindex = GB_tree_ES.score(X_train, y_train)\n",
    "\n",
    "print(\"Performance on test set\", round(cindex, 3))\n",
    "print(\"Performance on train set\", round(cindex_train, 3))\n",
    "print('CI_ipcw', mc.score_survival_model_ipcw(GB_tree_ES, X_test, y_train, y_test))\n",
    "\n",
    "\n",
    "improvement = pd.Series(\n",
    "    GB_tree_ES.oob_improvement_,\n",
    "    index=np.arange(1, 1 + len(GB_tree_ES.oob_improvement_))\n",
    ")\n",
    "ax = improvement.plot(xlabel=\"iteration\", ylabel=\"oob improvement\")\n",
    "ax.axhline(0.0, linestyle=\"--\", color=\"gray\")\n",
    "cutoff = len(improvement) - monitor.max_iter_without_improvement\n",
    "ax.axvline(cutoff, linestyle=\"--\", color=\"C3\")\n",
    "\n",
    "_ = improvement.rolling(monitor.window_size).mean().plot(ax=ax, linestyle=\":\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the early stopping model to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Xtest = GB_tree_ES.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = y_test_pre.copy()\n",
    "prediction['pred_X_test'] = pred_Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white')\n",
    "sns.scatterplot(x='F_Time', y='pred_X_test', hue='Observed', data=prediction,\n",
    "               alpha=0.6, palette=sns.xkcd_palette(['marine blue', 'deep red'])\n",
    "               )\n",
    "plt.plot([0, 3500000], [0, 3500000], 'darkgray', lw=0.8)\n",
    "plt.xlabel('Observed survival time from NDE measurement')\n",
    "plt.ylabel('Predicted survival time')\n",
    "plt.title('Gradient boosting with regression trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_real = GB_tree_ES.predict(real_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CI:', GB_tree_ES.score(real_x, real_y), '\\n'\n",
    "     'CI_ipcw:', mc.score_survival_model_ipcw(GB_tree_ES, real_x, y_train, real_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_real = real_y_pre.copy()\n",
    "prediction_real['prediction'] = pred_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3e}'.format\n",
    "prediction_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white')\n",
    "sns.scatterplot(x='F_Time', y='prediction', hue='Observed', data=prediction_real,\n",
    "               alpha=0.6, palette=sns.xkcd_palette(['marine blue', 'deep red'])\n",
    "               )\n",
    "plt.plot([0, 3500000], [0, 3500000], 'darkgray', lw=0.8)\n",
    "plt.xlabel('Observed survival time from NDE measurement')\n",
    "plt.ylabel('Predicted survival time')\n",
    "plt.title('Gradient boosting with RT - original data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(GB_tree_ES.feature_importances_, index=X_test.columns.tolist())\\\n",
    ".sort_values(0,ascending=True).plot.barh(color=[sns.color_palette('PuBu', 13, desat=0.9)], width=0.6, figsize=(6,6), legend=False)\n",
    "plt.xlabel('Feature importance', fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(GB_tree_ES, n_iter=15)\n",
    "perm.fit(X_test, y_test)\n",
    "feature_names = X_test.columns.tolist()\n",
    "eli5.explain_weights(perm, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting with component-wise least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "              'n_estimators': [4000, 5000, 6000],\n",
    "              'subsample': [0.1, 0.2, 0.3],\n",
    "             }\n",
    "GSCV_IPCWLS = GridSearchCV(ComponentwiseGradientBoostingSurvivalAnalysis(loss='ipcwls', random_state=42),\n",
    "                           param_grid, scoring=mc.score_survival_model,\n",
    "                   n_jobs=4, refit=False,\n",
    "                   cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSCV_IPCWLS.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(GSCV_IPCWLS.best_score_, 3), GSCV_IPCWLS.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_CWLS = ComponentwiseGradientBoostingSurvivalAnalysis(\n",
    "    loss='ipcwls',\n",
    "    subsample=1,\n",
    "    n_estimators=5000,\n",
    "    learning_rate=1, \n",
    "    dropout_rate=0.0,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_CWLS.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing several n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_cwls =  pd.DataFrame()\n",
    "for n_estimators in range(1, 5000, 100):\n",
    "    GB_CWLS.set_params(n_estimators=n_estimators)\n",
    "    GB_CWLS.fit(X_train, y_train)\n",
    "    results_cwls = pd.DataFrame({'n_estimators': n_estimators,\n",
    "                           'ci_train': GB_CWLS.score(X_train, y_train),\n",
    "                            'ci_test': GB_CWLS.score(X_test, y_test)\n",
    "                           }, index=[0])\n",
    "    scores_cwls = scores_cwls.append(results_cwls, ignore_index=True)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting vs n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid')\n",
    "scores_cwls.set_index('n_estimators').plot()\n",
    "plt.xlabel('n_estimator')\n",
    "plt.ylabel('concordance index')\n",
    "plt.title('Gradient boosting with component-wise least squares')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [i * 5 for i in range(1, 41)]\n",
    "\n",
    "estimators = {\n",
    "    \"no regularization\": ComponentwiseGradientBoostingSurvivalAnalysis(\n",
    "        learning_rate=1.0, random_state=0\n",
    "    ),\n",
    "    \"learning rate\": ComponentwiseGradientBoostingSurvivalAnalysis(\n",
    "        learning_rate=0.1, random_state=0\n",
    "    ),\n",
    "    \"dropout\": ComponentwiseGradientBoostingSurvivalAnalysis(\n",
    "        learning_rate=1.0, dropout_rate=0.1, random_state=0\n",
    "    ),\n",
    "    \"subsample\": ComponentwiseGradientBoostingSurvivalAnalysis(\n",
    "        learning_rate=1.0, subsample=0.5, random_state=0\n",
    "    ),\n",
    "}\n",
    "\n",
    "scores_reg_cwls = {k: [] for k in estimators.keys()}\n",
    "scores_train_reg_cwls = {k: [] for k in estimators.keys()}\n",
    "\n",
    "for n in n_estimators:\n",
    "    for name, est in estimators.items():\n",
    "        est.set_params(n_estimators=n)\n",
    "        est.fit(X_train, y_train)\n",
    "        cindex_cwls = est.score(X_test, y_test)\n",
    "        cindex_train_cwls = est.score(X_train, y_train)\n",
    "        scores_reg_cwls[name].append(cindex_cwls)\n",
    "        scores_train_reg_cwls[name].append(cindex_train_cwls)\n",
    "        \n",
    "scores_res_cwls = pd.DataFrame(scores_reg_cwls, index=n_estimators)\n",
    "scores_train_reg_cwls = pd.DataFrame(scores_train_reg_cwls, index=n_estimators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = scores_res_cwls.plot(xlabel=\"n_estimators\", ylabel=\"concordance index\")\n",
    "ax.grid(True)\n",
    "plt.title('Test')\n",
    "#plt.ylim(0.84, 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = scores_train_reg_cwls.plot(xlabel=\"n_estimators\", ylabel=\"concordance index\")\n",
    "ax.grid(True)\n",
    "plt.title('Train')\n",
    "#plt.ylim(0.84, 0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Xtest_cwls = GB_CWLS.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CI:', GB_CWLS.score(X_test, y_test), '\\n'\n",
    "     'CI_ipcw:', mc.score_survival_model_ipcw(GB_CWLS, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cwls = y_test_pre.copy()\n",
    "prediction_cwls['pred_X_test'] = pred_Xtest_cwls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white')\n",
    "sns.scatterplot(x='F_Time', y='pred_X_test', hue='Observed', data=prediction_cwls,\n",
    "               alpha=0.6, palette=sns.xkcd_palette(['marine blue', 'deep red'])\n",
    "               )\n",
    "plt.plot([0, 3500000], [0, 3500000], 'darkgray', lw=0.8)\n",
    "plt.xlabel('Observed survival time from NDE measurement')\n",
    "plt.ylabel('Predicted survival time')\n",
    "plt.title('Gradient boosting with IPCWLS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_real_cwls = GB_CWLS.predict(real_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CI:', GB_CWLS.score(real_x, real_y), '\\n'\n",
    "     'CI_ipcw:', mc.score_survival_model_ipcw(GB_CWLS, real_x, y_train, real_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_real_cwls = real_y_pre.copy()\n",
    "prediction_real_cwls['prediction'] = pred_real_cwls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3e}'.format\n",
    "prediction_real_cwls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white')\n",
    "sns.scatterplot(x='F_Time', y='prediction', hue='Observed', data=prediction_real_cwls,\n",
    "               alpha=0.6, palette=sns.xkcd_palette(['marine blue', 'deep red'])\n",
    "               )\n",
    "plt.plot([0, 3500000], [0, 3500000], 'darkgray', lw=0.8)\n",
    "plt.xlabel('Observed survival time from NDE measurement')\n",
    "plt.ylabel('Predicted survival time')\n",
    "plt.title('Gradient boosting with RT - original data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(GB_CWLS.coef_[1:], index=X_test.columns.tolist())\\\n",
    ".sort_values(0,ascending=True).plot.barh(color=[sns.color_palette('coolwarm', 13, desat=0.9)], width=0.6, figsize=(6,6), legend=False)\n",
    "plt.xlabel('Coefficients', fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(GB_CWLS, n_iter=15)\n",
    "perm.fit(X_test, y_test)\n",
    "feature_names = X_test.columns.tolist()\n",
    "eli5.explain_weights(perm, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:EPRI]",
   "language": "python",
   "name": "conda-env-EPRI-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
