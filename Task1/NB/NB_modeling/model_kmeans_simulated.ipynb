{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run K Means on simulated data\n",
    "\n",
    "Plot PCA of features\n",
    "\n",
    "Plot K means for all features (pairplot)\n",
    "\n",
    "Plot Kmeans on PCA reduced\n",
    "\n",
    "Determine cluster from elbow plot (inertia, automatic)\n",
    "\n",
    "Choose K visually\n",
    "\n",
    "Test minimal features\n",
    "\n",
    "Plot feature importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "import os, glob, inspect, sys\n",
    "\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "import epri_mc_lib as mc\n",
    "from importlib import reload\n",
    "reload(mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering with simulated data\n",
    "\n",
    "As an alternative to handling the uncertainty of the measurements mathematically when clustering, we can use data that was simulated to reflect the uncertainty of the measurements to train the model on. This should lead to a similar result even though it uses a different method. This also allows us to use more classical approaches to clustering and cluster evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "\n",
    "This data was simulated with 1000 replicates per condition based on the observed data. Details can be found in the notebook NB/NB_modeling/sample_generation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = \"../../Data/Merged_data\"\n",
    "df = pd.read_csv(os.path.join(data_path, 'ALL_TUBE_PIPE_simulated.csv'), \n",
    "                 index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating new values\n",
    "\n",
    "The AUC was calculated and the parameters were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"AUC_avg\"] = mc.findAUC(df, df['A'], df['B'], df['p'])\n",
    "df.drop(columns=[\"A\",\"B\",\"p\",'Absorption_avg_500','Absorption_avg_200'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional calculate the CF/perm ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['CF_perm'] = df['mean_CF']/df['mean_perm'].astype('float64')\n",
    "# df.drop(columns=[\"mean_MBN\",\"mean_perm\",\"mean_CF\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling values and selecting subsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_known = df.iloc[8000:,]\n",
    "scaled_known, scaler_known = mc.scale_general(df_known, MinMaxScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = mc.scale_general(df, MinMaxScaler())[0]\n",
    "\n",
    "tube, pipe, scaled_known, scaled_unknown = mc.get_subsample_df(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df['CF_perm'] = scaled_df['mean_CF']/scaled_df['mean_perm'].astype('float64')\n",
    "\n",
    "corr_scaled_df = scaled_df.copy().loc[:,mc.correlation_list]\n",
    "tube_scaled_corr, pipe_scaled_corr, \\\n",
    "tube_wo_blind_scaled_corr, tube_blind_scaled_corr = mc.get_subsample_df(corr_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_scaled_df = scaled_df.copy().loc[:,mc.minimal_informative_features]\n",
    "tube_scaled_mini, pipe_scaled_mini, \\\n",
    "tube_wo_blind_scaled_mini, tube_blind_scaled_mini = mc.get_subsample_df(mini_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_df = df.copy().loc[:,mc.minimal_informative_features]\n",
    "tube_mini, pipe_mini, \\\n",
    "tube_wo_blind_mini, tube_blind_mini = mc.get_subsample_df(mini_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of PCA\n",
    "\n",
    "To see what the uncertainty of the data looks like in terms of their distribution, principal component analysis was done with the simulated data and the first two components were plotted followed by the third and fourth components. The Last two components only explain a small amount of the variation. First this is done for the known tubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4, svd_solver='full')\n",
    "pca.fit(scaled_known)\n",
    "\n",
    "color_dict = { 'T_AR':'red', 'T_N':'blue', 'T_N_T':'black', 'T_T':'green','T_OT':'purple',\n",
    "             'T_FF':'grey', 'T_HAZ':'orange', 'T_HAZ_T':'yellow' }\n",
    "\n",
    "mc.biplot(pca, scaled_known, 0, 1, \"PCA biplot, Tubes (Known)\", color_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.biplot(pca, scaled_known, 2, 3, \"PCA biplot, Tubes (Known)\", color_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we repeat for the unknown tubes, which were transformed using the same PCA fit as the known tubes. There seems to be 3 samples that cannot be told apart, but there are possible identifications for the others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.transform(scaled_unknown)\n",
    "\n",
    "color_dict = { 'T_B1':'red', 'T_B2':'blue', 'T_B3':'black', 'T_B4':'green','T_B5':'purple',\n",
    "             'T_B6':'grey', 'T_B7':'orange', 'T_B8':'yellow' }\n",
    "\n",
    "mc.biplot(pca, scaled_unknown, 0, 1, \"PCA biplot, Tubes (Unknown)\", color_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.biplot(pca, scaled_unknown, 2, 3, \"PCA biplot, Tubes (Unknown)\", color_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the blind microstructure samples that were identified based on a single measurement in the previous reports, 4 are identified using this method, 1 additional sample is potentially identified, and one previously identified sample could not be identified. But is should be noted that this is based on the first two principal components alone so a full  model would presuably have more power.\n",
    "\n",
    "In agreement with previous reports:\n",
    "* FF=B7\n",
    "* OT=B8\n",
    "* N=B4\n",
    "* HAZ=B6\n",
    "\n",
    "Identified in previous report but not here:\n",
    "* AR=B5\n",
    "\n",
    "Identified here but not in previous reports:\n",
    "* N_T=B2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method\n",
    "\n",
    "First this tries to find a reasonable k automatically in the classic way. This doesn't work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_range = 2\n",
    "max_range = 8\n",
    "\n",
    "def plot_elbow_kmeans(feat_norm, title):\n",
    "    '''\n",
    "    Elbow plot\n",
    "    Args:\n",
    "    - feat_norm : pandas dataframe\n",
    "    - title : title of the figure ideally correpond to the samples\n",
    "    return plot\n",
    "    '''\n",
    "    \n",
    "    inertia = []\n",
    "    k_list = range(min_range, max_range+1)\n",
    "\n",
    "    for k in k_list:\n",
    "        km = KMeans(n_clusters = k, random_state= 0)\n",
    "        km.fit(feat_norm) \n",
    "        score = km.inertia_\n",
    "        inertia.append(score)\n",
    "\n",
    "\n",
    "    plt.figure(1 , figsize = (10 ,6))\n",
    "    plt.plot(np.arange(min_range , max_range+1) , inertia , 'o')\n",
    "    plt.plot(np.arange(min_range , max_range+1) , inertia , '-' , alpha = 0.5)\n",
    "\n",
    "    plt.xlabel('Number of Clusters', fontsize=20) , plt.ylabel('Inertia', fontsize=20)\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_elbow_kmeans(tube_wo_blind_scaled_mini, title='Identified tubes minimal features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_elbow_kmeans(tube_wo_blind_scaled_corr, title='Identified tubes selected features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto find K\n",
    "Source: https://jtemporal.com/kmeans-and-elbow-method/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wcss(data):\n",
    "    '''\n",
    "    Calculate within class sum-squared value which represents loss in KMeans clustering\n",
    "    '''\n",
    "    wcss = []\n",
    "    for n in range(min_range, max_range):\n",
    "        kmeans = KMeans(n_clusters=n,random_state=0)\n",
    "        kmeans.fit(data)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    \n",
    "    return wcss\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "def optimal_number_of_clusters(wcss):\n",
    "    '''\n",
    "    Calculate normal distance \n",
    "    '''\n",
    "    x1, y1 = min_range, wcss[0]\n",
    "    x2, y2 = max_range, wcss[len(wcss)-1]\n",
    "\n",
    "    distances = []\n",
    "    for i in range(len(wcss)):\n",
    "        x0 = i+2\n",
    "        y0 = wcss[i]\n",
    "        numerator = abs((y2-y1)*x0 - (x2-x1)*y0 + x2*y1 - y2*x1)\n",
    "        denominator = sqrt((y2 - y1)**2 + (x2 - x1)**2)\n",
    "        distances.append(numerator/denominator)\n",
    "    \n",
    "    return distances.index(max(distances)) + 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the within clusters sum-of-squares for n cluster amounts\n",
    "sum_of_squares = calculate_wcss(tube_wo_blind_scaled_corr)\n",
    "    \n",
    "# calculating the optimal number of clusters\n",
    "n = optimal_number_of_clusters(sum_of_squares)\n",
    "print('Number of cluster =', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the within clusters sum-of-squares for n cluster amounts\n",
    "sum_of_squares = calculate_wcss(tube_wo_blind_scaled_mini)\n",
    "    \n",
    "# calculating the optimal number of clusters\n",
    "n = optimal_number_of_clusters(sum_of_squares)\n",
    "print('Number of cluster for all tubes =', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot K-Means on sample distribution scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kmeans(df_scaled, df_ori, k):\n",
    "    '''\n",
    "    Scatter plot\n",
    "    Args:\n",
    "    - df : scaled pandas dataframe\n",
    "    - range_col : np.r_[range of column wanted]\n",
    "    return plot\n",
    "    '''\n",
    "    model = KMeans(n_clusters = k, random_state= 42)\n",
    "    model.fit(df_scaled) \n",
    "    labels = model.predict(df_scaled)\n",
    "    print(labels)\n",
    "    silhouette = metrics.silhouette_score(df_scaled, labels, metric='euclidean')\n",
    "    print(silhouette)\n",
    "    df_ori['labels'] = labels\n",
    "    sns.pairplot(df_ori, hue='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans(tube_wo_blind_scaled_mini, tube_wo_blind_mini, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing k visually\n",
    "\n",
    "The elbow method gives relatively low values of k even though more clusters are clerly separable based on the PCA visualization. Instead, this simply plots the PCA and colors the points based on the clustering with different values of k to see whether it can identify the actual conditions. In fact with k=6 the model is able to roughly identify the known clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA(n_components=0.9, svd_solver='full')\n",
    "pca.fit(scaled_known)\n",
    "\n",
    "color_dict = { 0:'cyan', 1:'burlywood', 2:'pink', 3:'silver', 4:'khaki', 5:'palegreen', 6:'steelblue', 7:'plum'}\n",
    "\n",
    "plot_scaled = scaled_known.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters = 6, random_state= 42)\n",
    "model.fit(scaled_known) \n",
    "labels = model.labels_\n",
    "plot_scaled.index = labels\n",
    "mc.biplot(pca, plot_scaled, 0, 1, \"K-means clustering, Tubes (Known)\", color=color_dict, plot_vectors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters = 7, random_state= 42)\n",
    "model.fit(scaled_known) \n",
    "labels = model.labels_\n",
    "plot_scaled.index = labels\n",
    "mc.biplot(pca, plot_scaled, 0, 1, \"K-means clustering, Tubes (Known)\", color=color_dict, plot_vectors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters = 8, random_state= 42)\n",
    "model.fit(scaled_known) \n",
    "labels = model.labels_\n",
    "plot_scaled.index = labels\n",
    "mc.biplot(pca, plot_scaled, 0, 1, \"K-means clustering, Tubes (Known)\", color=color_dict, plot_vectors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify original blind samples with k-means\n",
    "\n",
    "For this the original 8 samples of known and unknown tubes are classified with the model built using simulated data. This does not take into account the uncertainty of the blind data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../Data/Merged_data\"\n",
    "df_original = pd.read_csv(os.path.join(data_path, 'ALL_TUBE_PIPE_merge_1.csv'), \n",
    "                 index_col=0)\n",
    "df_original[\"AUC_avg\"] = mc.findAUC(df_original, df_original['A'], df_original['B'], df_original['p'])\n",
    "df_original.drop(columns=[\"median_CF\",\"median_perm\",\"median_MBN\",\"A\",\"B\",\"p\",'Absorption_avg_500','Absorption_avg_200']+mc.errors_list,inplace=True)\n",
    "df_original = df_original.iloc[:16,]\n",
    "df_original.dropna(axis=1, inplace=True)\n",
    "scaled_original = mc.scale_general(df_original,MinMaxScaler())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters = 6, random_state= 42)\n",
    "model.fit(scaled_known) \n",
    "model.predict(scaled_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters created are:\n",
    "\n",
    "* 0: N_T, B2\n",
    "* 1: N, \n",
    "* 2: AR, HAZ_T, T, B1, B3, B5\n",
    "* 3: FF, B7\n",
    "* 4: HAZ, B6, B4\n",
    "* 5: OT, B8\n",
    "\n",
    "In agreement with previous reports:\n",
    "* FF=B7\n",
    "* OT=B8\n",
    "* HAZ=B6\n",
    "\n",
    "Identified in previous report but not here:\n",
    "* AR=B5\n",
    "\n",
    "Identified here but not in previous reports:\n",
    "* N_T=B2\n",
    "\n",
    "Problems:\n",
    "* B4 corresponds to N but its uncertainty overlaps with HAZ and is incorrectly grouped there\n",
    "\n",
    "This methods handles well the uncertainty of the training data but does not handle the uncertainty of the prediction data of the blind tubes causing a misclassification. We may need a way to compare between two sample distributions instead. Reducing the uncertainty would also allow the clusters to be more easily separated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A minimal feature set\n",
    "\n",
    "Since many of the features are correlated and contribute similarly to the PCA this tries to find a minimum that can recreate the same result. Features were individually dropped and their effect on the principal components were observed. The features with minimal effect on the components were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_set_known = scaled_known.copy()[[\"TEP_mean_uV_C\",\"Absorption_avg_50\",\"mean_perm\",\"AUC_avg\",\"backscatter_avg\"]]\n",
    "minimal_original = scaled_original[[\"TEP_mean_uV_C\",\"Absorption_avg_50\",\"mean_perm\",\"AUC_avg\",\"backscatter_avg\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4, svd_solver='full')\n",
    "pca.fit(minimal_set_known)\n",
    "\n",
    "color_dict = { 0:'cyan', 1:'burlywood', 2:'pink', 3:'silver', 4:'khaki', 5:'palegreen', 6:'steelblue', 7:'plum'}\n",
    "\n",
    "plot_scaled = minimal_set_known.copy()\n",
    "\n",
    "model = KMeans(n_clusters = 6, random_state= 42)\n",
    "model.fit(minimal_set_known) \n",
    "labels = model.labels_\n",
    "plot_scaled.index = labels\n",
    "mc.biplot(pca, plot_scaled, 0, 1, \"Minimal K-means clustering, Tubes (Known)\", color_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(minimal_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters are:\n",
    "\n",
    "* 0: B1, B3, B5, AR, HAZ_T, T\n",
    "* 1: B4, B6, HAZ\n",
    "* 2: B7, FF\n",
    "* 3: B2, N_T\n",
    "* 4: N\n",
    "* 5: B8, OT\n",
    "\n",
    "These predictions are identical to what was made with the full model. This may represent a minimal feature set that contains the majority of the information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "The first two principal components (and especially the first) explain the majority of the variance. The first component is largely made up of TEP and permeability. All the features except backscatter contribute to the second component and backscatter contributes to the third component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([\"PC1\",\"PC2\",\"PC3\",\"PC4\"],pca.explained_variance_ratio_, align='center', alpha=0.5, color=\"gray\")\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel(\"Explained variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame([x*abs(y) for x,y in zip(pca.explained_variance_ratio_, pca.components_)],columns=minimal_set_known.columns, index=[\"PC1\",\"PC2\",\"PC3\",\"PC4\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feat_imp = feature_importance.transpose().sort_values('PC1', ascending=False).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feat_imp.plot(kind='barh', color=sns.color_palette('PuBu_r', 5, desat=0.9), width=0.6, figsize=(6,6))\n",
    "plt.xlabel('Feature importance (explained variance ratio)', fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.15         # the width of the bars\n",
    "\n",
    "pca_components = [\"PC1\",\"PC2\",\"PC3\",\"PC4\"]\n",
    "\n",
    "for i in range(5):\n",
    "    ax.bar(ind + width*i, feature_importance.iloc[:,i], width, label=feature_importance.columns[i])\n",
    "\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels(pca_components)\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel('PCA components scaled by explained variance')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.iloc[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
