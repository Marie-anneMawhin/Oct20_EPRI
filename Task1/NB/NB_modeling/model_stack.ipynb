{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a stack of different clusetring algorithms from the skealrn library\n",
    "\n",
    "calculate the cophenetic coefficient for agglomerative algorithm\n",
    "\n",
    "print silhouette score of different models\n",
    "\n",
    "plot results on PCA reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "import os, glob, inspect, sys, time, warnings\n",
    "\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "import epri_mc_lib as mc\n",
    "from importlib import reload\n",
    "reload(mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = \"../../Data/Merged_data\"\n",
    "df = pd.read_csv(os.path.join(data_path, 'ALL_TUBE_PIPE_simulated.csv'), \n",
    "                 index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AUC_avg'] = mc.findAUC(df, A=df['A'], B=df['B'], p=df['p'], name='AUC_avg')\n",
    "df.drop(columns=[\"A\",\"B\",\"p\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['CF_perm'] = df['mean_CF']/df['mean_perm'].astype('float64')\n",
    "#df.drop(columns=[\"mean_MBN\",\"mean_CF\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Absorption_avg_500\",\"Absorption_avg_200\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = mc.scale_general(df, MinMaxScaler())[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting sub samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tube, pipe, tube_wo_blind, tube_blind = mc.get_subsample_df(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df['CF_perm'] = scaled_df['mean_CF']/scaled_df['mean_perm'].astype('float64')\n",
    "corr_scaled_df = scaled_df.copy().loc[:,mc.correlation_list]\n",
    "tube_scaled_corr, pipe_scaled_corr, \\\n",
    "tube_wo_blind_scaled_corr, tube_blind_scaled_corr = mc.get_subsample_df(corr_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_scaled_df = scaled_df.copy().loc[:,mc.minimal_informative_features]\n",
    "tube_scaled_mini, pipe_scaled_mini, \\\n",
    "tube_wo_blind_scaled_mini, tube_blind_scaled_mini = mc.get_subsample_df(mini_scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cophenetic Correlation Coefficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The closer the value is to 1, the better the clustering preserves the original distances.\n",
    "def get_c (df):\n",
    "    results = []\n",
    "    for linkage_name in ['single', 'average', 'weighted', 'centroid', 'median', 'ward']:\n",
    "        for metric_name in ['chebyshev', 'cityblock', 'cosine', 'euclidean', 'minkowski', 'sqeuclidean']:\n",
    "            try:\n",
    "                Z = hierarchy.linkage(df, method=linkage_name, metric=metric_name)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            c, coph_dists = hierarchy.cophenet(Z, pdist(df, metric_name))\n",
    "\n",
    "            results.append([linkage_name, metric_name, c])\n",
    "    data = pd.DataFrame(results, columns=['linkage', 'distance metric', 'C']).sort_values('C', ascending=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_c(tube_wo_blind_scaled_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **MeanShift**: [`MeanShift`](https://sklearn.org/modules/generated/sklearn.cluster.MeanShift.html#sklearn.cluster.MeanShift)\n",
    "\n",
    "- **K-Means**:\n",
    "[`KMeans`](https://sklearn.org/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)\n",
    "\n",
    "- **Agglomerative Hierarchical Clustering**:\n",
    "[`AgglomerativeClustering`](https://sklearn.org/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering)\n",
    "\n",
    "- **Ward**:\n",
    "[`AgglomerativeClustering`](https://sklearn.org/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering)\n",
    "\n",
    "- **Spectral Clustering**:\n",
    "[`SpectralClustering`](https://sklearn.org/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering)\n",
    "\n",
    "- **DBSCAN**:\n",
    "[`DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.dbscan.html)\n",
    "    \n",
    "- **OPTICS**:\n",
    "[`OPTICS`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html)\n",
    "\n",
    "- **BIRCH**:\n",
    "[`BIRCH`]([`BIRCH`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.dbscan.html)\n",
    "\n",
    "- **Gaussian Mixture**:\n",
    "[`GaussianMixture`](https://scikit-learn.org/stable/modules/mixture.html#gmm)\n",
    "\n",
    "- **Bayesian Gaussian Mixture**:\n",
    "[`BayesianGaussianMixture`](https://scikit-learn.org/stable/modules/mixture.html#gmm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "\n",
    "default_base = {'quantile': .1, #for bandwidth of RBF kernel in meanshift\n",
    "                'eps': 0.1, #max distance between 2 samples for DBSCAN\n",
    "                'metric': 'chebyshev', #for DBSCAN\n",
    "                'damping': .1, #for Affinity Propagation:the extent to which the current value is maintained relative to incoming values\n",
    "                'preference': -200, #for Affinity Propagation: each point - points with larger values of preferences are more likely to be chosen as exemplars.\n",
    "                'n_neighbors': 3, #for k-neighbors graph for the connectivity matrix\n",
    "                'n_clusters': 6, #for kmeans, ward, spectral, meanshift, BIRCH\n",
    "                #for kmeans determine on elbow cf model_kmeans_simulated.ipynb\n",
    "                'xi': 0.02, #for OPTICS Determines the minimum steepness on the reachability plot that constitutes a cluster boundary.\n",
    "                'min_cluster_size': 0.007, # for optics Minimum number of samples in an OPTICS cluster, expressed as an absolute number or a fraction of the number of samples\n",
    "                'threshold': 0.05, #for BIRCH to limit the samples per leaf node \n",
    "                'random_state': random_state\n",
    "               }\n",
    "\n",
    "datasets = [\n",
    "    #(tube, {})\n",
    "    #(tube_scaled_mini, {}),\n",
    "    #(tube_scaled_corr, {}),\n",
    "    (tube_wo_blind_scaled_mini, {})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X  = dataset\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "    # connectivity matrix for structured Ward (if need see jelly roll example)\n",
    "    connectivity = kneighbors_graph(X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth)\n",
    "    two_means = cluster.KMeans(n_clusters=params['n_clusters']) #use minibatch if too heavy\n",
    "    average_linkage = cluster.AgglomerativeClustering(n_clusters=params['n_clusters'], linkage='ward', connectivity=connectivity)\n",
    "    ward = cluster.AgglomerativeClustering(linkage=\"ward\", affinity=\"euclidean\", n_clusters=params['n_clusters'])\n",
    "    spectral = cluster.SpectralClustering(n_clusters=params['n_clusters'], eigen_solver='arpack', affinity=\"nearest_neighbors\") #can also be rbf\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'], metric=params['metric'])\n",
    "    #affinity_propagation = cluster.AffinityPropagation(damping=params['damping'], preference=params['preference'])\n",
    "    optics = cluster.OPTICS(xi=params['xi'], min_cluster_size=params['min_cluster_size'])\n",
    "    birch = cluster.Birch(n_clusters=params['n_clusters'], threshold=params['threshold'])\n",
    "    gmm = mixture.GaussianMixture(n_components=params['n_clusters'], covariance_type='full')\n",
    "    bgmm = mixture.BayesianGaussianMixture(n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('Kmeans', two_means),\n",
    "        #('AffinityPropagation', affinity_propagation), EXCLUDED because of high complexity, most appropriate for small to medium sized datasets\n",
    "        ('Ward', ward),\n",
    "        ('AgglomerativeClustering', average_linkage),\n",
    "        ('MeanShift', ms),\n",
    "        ('SpectralClustering', spectral),\n",
    "        ('DBSCAN', dbscan),\n",
    "        ('OPTICS', optics),\n",
    "        ('Birch', birch),\n",
    "        ('GaussianMixture', gmm),\n",
    "        ('BayesianGaussianMixture', bgmm)\n",
    "    )\n",
    "\n",
    "    labels = pd.DataFrame(index=X.index)\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"the number of connected components of the \" + \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",category=UserWarning)\n",
    "            warnings.filterwarnings(\"ignore\", message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\", category=UserWarning)\n",
    "            \n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        \n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "        \n",
    "        labels['label_' + name] = y_pred\n",
    "        \n",
    "        silhouette = metrics.silhouette_score(X, labels['label_' + name], metric='euclidean')\n",
    "        print(name, ': silhouette score = ',silhouette)\n",
    "    results = pd.concat([X, labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform PCA to plot all dimension (n_components = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "transformed = pca.fit_transform(X)\n",
    "results['PC 1'] = transformed[:,0]\n",
    "results['PC 2'] = transformed[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(int(len(datasets)*2), int(len(clustering_algorithms)/2), figsize=(20, 7))\n",
    "plt.tight_layout(h_pad=5, w_pad=5)\n",
    "for ax, col in zip(axes.flatten(), labels.columns):\n",
    "    print(ax, col)\n",
    "    sns.scatterplot(data=results, ax=ax,\n",
    "                    x=results['PC 1'], y=results['PC 2'], s=10, hue=col)\n",
    "    handles, lab = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles=handles[:], labels=lab[:])\n",
    "\n",
    "    ax.set_title(col.split('_')[1], size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
