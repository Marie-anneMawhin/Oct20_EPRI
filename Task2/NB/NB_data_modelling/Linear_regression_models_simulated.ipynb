{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression on simulated data (Linear, Ridge, ElasticNet, Lasso), no tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os, glob, inspect, sys\n",
    "import re\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "import epri_mc_lib_2 as mc\n",
    "from importlib import reload\n",
    "reload(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basic features importances:\n",
    "def get_feature_importance(name, model, feature_names):\n",
    "    '''return classical feature importances\n",
    "    Args:\n",
    "        -name:str\n",
    "        -model: trained model\n",
    "    return importance as a df    \n",
    "    '''\n",
    "    if name == 'Ridge' or name == 'Elastic' or name == 'Lasso':\n",
    "        importance = model.coef_\n",
    "        importance_df = pd.DataFrame(importance.T, columns=[name], index=feature_names)\n",
    "        importance_df.sort_values(name, ascending=True, inplace=True)\n",
    "       \n",
    "    if name == 'KNN' or name == 'SVM':\n",
    "        pass\n",
    "        \n",
    "    if name == 'RF' or name == 'Tree': \n",
    "        importance = model.feature_importances_\n",
    "        rel_importance = 100.0 * (importance / importance.sum())\n",
    "        importance_df = pd.DataFrame(rel_importance.T, columns=[name], index=feature_names)\n",
    "        importance_df.sort_values(name, ascending=True, inplace=True)\n",
    " \n",
    "    if name == 'XGB':\n",
    "        importance = model.feature_importances_\n",
    "        rel_importance = 100.0 * (importance / importance.sum())\n",
    "        importance_df = pd.DataFrame(rel_importance.T, columns=['XGB'], index=feature_names)\n",
    "        importance_df.sort_values('XGB', ascending=True, inplace=True)\n",
    "         \n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data and merge replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_simulated = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), '../Data/Merged_data/CopulaGAN_simulated_data_up.csv'),\n",
    "                    index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = merged_simulated.copy()\n",
    "cw_regex = re.compile(\"[0-9]+$\") \n",
    "cold_work = [str(re.search(cw_regex,x).group()) for x in mean_df.type_cw ]\n",
    "mean_df['KJIC'] = mean_df.index\n",
    "cw_regex = re.compile(\"^[A]*[0-9]+\") \n",
    "mean_df.index = [str(re.search(cw_regex,x).group()) for x in mean_df.type_cw ]\n",
    "mean_df = mean_df[[\"KJIC\",\"MS_Avg\",\"TEP_average\",\"Beta_avg\",\"IF_amp_2.25MHz\",\"IF_amp_3.5MHz\",\"BS_amp\"]]\n",
    "mean_df['log_MS_Avg'] = np.log(mean_df['MS_Avg'])\n",
    "mean_df['log_beta_avg'] = np.log(mean_df['Beta_avg']) \n",
    "log_kjic = np.log(mean_df.KJIC)\n",
    "mean_kjic = mean_df.KJIC\n",
    "mean_df.drop(columns=['KJIC','MS_Avg','Beta_avg'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = mc.scale_general(mean_df, MinMaxScaler())[0]\n",
    "scaled_df.index = mean_df.index\n",
    "#scaled_df[\"cold_work\"] = cold_work\n",
    "# The logarithmic of the KJIC is incorporated for better results\n",
    "scaled_kjic = mc.scale_general(pd.DataFrame(mean_kjic), MinMaxScaler())[0]\n",
    "scaled_kjic.index = mean_df.index\n",
    "scaled_df['KJIC'] = scaled_kjic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scaled_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating data depending on the type of steel\n",
    "SS_304=scaled_df[scaled_df.index == '304']\n",
    "SS_316=scaled_df[scaled_df.index == '316']\n",
    "SS_347=scaled_df[scaled_df.index == '347']\n",
    "SS_A286=scaled_df[scaled_df.index == 'A286']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_list=[\"SS304\",\"SS316\",\"SS347\",\"SSA286\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_sstype(scaled_df,name):\n",
    "    # Creating Predictor variable 'X' and Target Variable 'y'\n",
    "    # X contains all the features except for the target value Price\n",
    "    X = scaled_df.drop('KJIC', axis = 1)\n",
    "    y = scaled_df['KJIC']\n",
    "    \n",
    "    # Creating the training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=50)\n",
    "    \n",
    "    # X_train contains 70% of total dataset\n",
    "    print(\"Training dataset:\", X_train.shape)\n",
    "    # X_test contains 30% of total dataset\n",
    "    print(\"Test dataset:\", X_test.shape)\n",
    "    \n",
    "    # Model Liner Regression\n",
    "    lr = LinearRegression()\n",
    "    \n",
    "    #Train/fit the model to training data\n",
    "    lr.fit(X_train,y_train)\n",
    "    \n",
    "    pred = lr.predict(X_test)\n",
    "    \n",
    "    # print the intercept\n",
    "    print(\"Intercept:\", lr.intercept_)\n",
    "    \n",
    "    #Coefficients\n",
    "    coeff_df = pd.DataFrame(lr.coef_,X.columns,columns=['Coefficient'])\n",
    "    print(\"Coefficients:\", coeff_df)\n",
    "    \n",
    "    \n",
    "    print(\"r2 score for training: \", r2_score(y_train, lr.predict(X_train)))\n",
    "    print(\"r2 score for testing: \", r2_score(y_test, pred))\n",
    "    \n",
    "    print('MAE:', metrics.mean_absolute_error(y_test, pred))\n",
    "    print('MSE:', metrics.mean_squared_error(y_test, pred))\n",
    "    print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))\n",
    "    \n",
    "    plt.scatter(y_test,pred)\n",
    "    plt.title(\"Linear Regression of \"+ name)\n",
    "    plt.xlabel(\"Fracture Toughness\")\n",
    "    plt.ylabel(\"Predicted Fracture Toughness\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sstype(SS_304,\"SS304\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_sstype(scaled_df,name):\n",
    "    # Creating Predictor variable 'X' and Target Variable 'y'\n",
    "    # X contains all the features except for the target value Price\n",
    "    X = scaled_df.drop('KJIC', axis = 1)\n",
    "    y = scaled_df['KJIC']\n",
    "    \n",
    "    # Creating the training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=50)\n",
    "    \n",
    "    # X_train contains 70% of total dataset\n",
    "    print(\"Training dataset:\", X_train.shape)\n",
    "    # X_test contains 30% of total dataset\n",
    "    print(\"Test dataset:\", X_test.shape)\n",
    "    \n",
    "    #Model Ridge Rigression\n",
    "    rr = Ridge(alpha=0.01)\n",
    "    rr.fit(X_train, y_train) \n",
    "    pred_train_rr= rr.predict(X_train)\n",
    "    print(\"RMSE train:\", np.sqrt(mean_squared_error(y_train,pred_train_rr)))\n",
    "    print(\"r2 score for training: \", r2_score(y_train, pred_train_rr))\n",
    "    \n",
    "    pred_test_rr= rr.predict(X_test)\n",
    "    print(\"RMSE test:\", np.sqrt(mean_squared_error(y_test,pred_test_rr))) \n",
    "    print(\"r2 score for test: \", r2_score(y_test, pred_test_rr))\n",
    "    \n",
    "    plt.scatter(y_test,pred_test_rr)\n",
    "    plt.plot([0, 1], [0, 1])\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.title(\"Ridge Regression of \"+ name)\n",
    "    plt.xlabel(\"Fracture Toughness\")\n",
    "    plt.ylabel(\"Predicted Fracture Toughness\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_sstype(SS_304,\"SS304\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_sstype(SS_316,\"SS316\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_sstype(SS_347,\"SS347\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enet_sstype(scaled_df,name):\n",
    "    # Creating Predictor variable 'X' and Target Variable 'y'\n",
    "    # X contains all the features except for the target value Price\n",
    "    X = scaled_df.drop('KJIC', axis = 1)\n",
    "    y = scaled_df['KJIC']\n",
    "    \n",
    "    # Creating the training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=50)\n",
    "    \n",
    "    # X_train contains 70% of total dataset\n",
    "    print(\"Training dataset:\", X_train.shape)\n",
    "    # X_test contains 30% of total dataset\n",
    "    print(\"Test dataset:\", X_test.shape)\n",
    "    \n",
    "    #Model Ridge Rigression\n",
    "    model_enet = ElasticNet(alpha = 0.01)\n",
    "    model_enet.fit(X_train, y_train) \n",
    "    pred_train_enet= model_enet.predict(X_train)\n",
    "    print(\"RMSE train:\", np.sqrt(mean_squared_error(y_train,pred_train_enet)))\n",
    "    print(\"r2 score for training: \", r2_score(y_train, pred_train_enet))\n",
    "    \n",
    "    pred_test_enet= model_enet.predict(X_test)\n",
    "    print(\"RMSE test:\", np.sqrt(mean_squared_error(y_test,pred_test_enet)))\n",
    "    print(\"r2 score for test: \", r2_score(y_test, pred_test_enet))\n",
    "    \n",
    "    plt.scatter(y_test,pred_test_enet)\n",
    "    plt.plot([0, 1], [0, 1])\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.title(\"Elastic Net Regression of \"+ name)\n",
    "    plt.xlabel(\"Fracture Toughness\")\n",
    "    plt.ylabel(\"Predicted Fracture Toughness\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_sstype(SS_304,\"SS304\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_sstype(SS_316,\"SS316\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_sstype(SS_347,\"SS347\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_sstype(scaled_df,name, alpha):\n",
    "    # Creating Predictor variable 'X' and Target Variable 'y'\n",
    "    # X contains all the features except for the target value Price\n",
    "    X = scaled_df.drop('KJIC', axis = 1)\n",
    "    y = scaled_df['KJIC']\n",
    "    \n",
    "    # Creating the training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=50)\n",
    "    \n",
    "    print(name+\", \"+str(alpha))\n",
    "    # X_train contains 70% of total dataset\n",
    "    print(\"Training dataset:\", X_train.shape)\n",
    "    # X_test contains 30% of total dataset\n",
    "    print(\"Test dataset:\", X_test.shape)\n",
    "    \n",
    "    #Model LASSO Rigression\n",
    "    model_lasso = Lasso(alpha=alpha)\n",
    "    model_lasso.fit(X_train, y_train) \n",
    "    pred_train_lasso= model_lasso.predict(X_train)\n",
    "    print()\n",
    "    print(\"RMSE train:\", np.sqrt(mean_squared_error(y_train,pred_train_lasso)))\n",
    "    print(\"r2 score for train: \", r2_score(y_train, pred_train_lasso))\n",
    "    \n",
    "    pred_test_lasso= model_lasso.predict(X_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test,pred_test_lasso))\n",
    "    r2_test = r2_score(y_test, pred_test_lasso)\n",
    "    print(\"RMSE test:\", np.sqrt(mean_squared_error(y_test,pred_test_lasso))) \n",
    "    print(\"r2 score for test: \", r2_score(y_test, pred_test_lasso))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(y_test,pred_test_lasso)\n",
    "    plt.plot([0, 1], [0, 1])\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.title(\"LASSO Regression of \"+ name+\", \"+str(alpha))\n",
    "    plt.text(0.1, 0.90, 'RMSE: '+str(round(rmse_test,3)))\n",
    "    plt.text(0.1, 0.82, '  R^2: '+str(round(r2_test,3)))\n",
    "    plt.xlabel(\"Fracture Toughness\")\n",
    "    plt.ylabel(\"Predicted Fracture Toughness\")\n",
    "    \n",
    "    plt.figure()\n",
    "    classic = get_feature_importance(\"Lasso\", model_lasso, X_train.columns)\n",
    "    classic.plot.barh(figsize=(5,5), color=[sns.color_palette(palette='PuBu', n_colors=len(X_train.columns))], \n",
    "        legend=False, title=name+\", \"+str(alpha) )\n",
    "    plt.xlabel('coefficients')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in [0.01, 0.002, 0.001]:\n",
    "    lasso_sstype(SS_304,\"SS304\", alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in [0.01, 0.002, 0.001]:\n",
    "    lasso_sstype(SS_316,\"SS316\", alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in [0.01, 0.002, 0.001]:\n",
    "    lasso_sstype(SS_347,\"SS347\", alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
